---
title: "R and ggplot2"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

===

# Start of class notes #

I left the previous section in just to give you the sample of what the 
initial R markdown document is.

# R #

R is a functional language primarily used for statistics. It is most
similar to SAS and Tableau.

- This is the focal point for the statistics community in sharing code
    - Most advanced statistics algorithms are implemented in R
- They community has a good open source repository
    - CRAN - the R interpreter is automatically integrated with CRAN
- RStudio provides steroids for R with
    - Interactive editing and debugging environment
    - Automatic integration with CRAN
    - Hyperlink help browser
    - Plot viewer integration
    - RMarkdown support (like Jupyter)
    - Shiny support (like Bokeh)

# RStudio #

I really like RStudio/R markdown/knitr and there are features that I really wish
I had in Jupyter. For example:

- the package installer to CRAN
    - the command line to install Anaconda packages is pretty good
    - but outside of Anaconda, pip and PyPI is horrible in comparison to CRAN 
        - R is far more user friendly than Python
- the ability to save a document immediately that is statically viewable
    - again, you can do this from the command line with Jupyter notebooks
        - but Rstudio again is more user friendly
- literate programming through interspersed markdown and R
    - once again, you have markdown cells and code cells in Jupyter, but it's not seemless
        - it makes code documentation very easy
        - to me, it feels more natural to be typing inline code and then inline comments
            - it's a little jarring in Jupyter to switch between markdown and code cells
- help viewer
    - you can do help() and dir() anywhere in a Python cell, but the hyperlink
      viewer is nicer in Rstudio

# R, RStudio, CRAN #

They are the products of a great open source community that is really connected.
  
Let's visualize some data...

First, we need to load dplyr.

```{r dplyr, echo=FALSE}
require(dplyr)
```

dplyr is that DSL (similar to LINQ) that I was talking about 
for relational data. We'll use it to load our can as we did
before, and I'll show how it works to compose queries
on your data sets.

Let's load the can again.

```{r load-can}
db <- src_sqlite("can.db")
t <- tbl(db, "can")
```

t is our table with the data.

```{r show-table}
t
```

Now, we can start selecting data from our table.

Note, that t is not actually loaded into memory, but it is a reference to our SQLite database. This is a difference from data frames, where a typical data frame (both in R and Python) are in memory. dplyr is "lazily evaluated" and only fetches data from disk when you either display it or `collect()` it.

The way we select data with dplyr is through functional combinators aka pipelines aka data flow graphs.

```{r select-data-1}
t %>% filter(Points_2 > -6) %>% filter(Points_2 < -4)
```

The "%>%" symbol is pipe, you can think of it the same as a Unix pipe, "|".
It is piping the data from the previous command, to the next.

What this small little program fragment says is:

- load table t
- filter the previous step where Points_2 > -6
- filter the previous step where Points_2 < -4
- (display the data)

We've selected the points that are between -6 < z < -4.

The interesting thing is that our little functional program is
being transformed into SQL code, and that it is streaming/being executed
on disk. The data is never loaded into memory unless you force it
to (with collect()).

Let's plot the data to show you that they really are the same as before. 
We'll use ggplot2.

```{r load-ggplot}
require(ggplot2)

t %>% 
  filter(Points_2 > -6) %>% 
  filter(Points_2 < -4) %>% 
  collect() %>% 
  ggplot(aes(Points_0, Points_2, colour=Points_1)) + 
  geom_point()
```

Here, we've added an additional pipe to our data to transform
it into graphics, where "+" is the pipe operator for ggplot2
functions.

What this little program says now is:

- load table t
- filter the previous step where Points_2 > -6
- filter the previous step where Points_2 < -4
- collect the previous step into memory
- transform the previous step into an x-y plot
- apply point primitives to the x-y plot
- (display the data)

# Relationship between data flow, relational stores, functional combinators, and data parallelism #

The "gg" in ggplot stands for the grammar of graphics. It
is a functional combinator way of describing graphics, as well,
such that you pipe graphics primitives to transformers until
you get the final output.

You can think of the entire pipeline from data to picture is:

`data => data transformers => graphics transformers => picture`

where dplyr is our set of data transformers, and ggplot2
is our set of graphics transformers.

Graphics and visualization treats data in a data parallel way,
such that data flows through a program and is transformed
along the way. Examples of where the graph is explicit can
be found in data flow programming tools, such as LabView,
Simulink, Pure Data, Max/MSP, ParaView, VTK, etc. Generally,
"state" or "values" in a graphics pipeline is immutable, and it
is transformed along the way. 

This is evident in how the graphics pipeline is taught, how
ParaView and VTK represent the program as a directed acyclic
graph (DAG), how OpenGL represents transforms on graphics
primitives, how Cuda and GPUs treat data as vectors, etc.
Hopefully, you can see also there is a relationship between
functional programming and the relational data model as well.

Let's do one more example.

```{r one-more}
t %>% 
  mutate(vel = VEL_0*VEL_0+VEL_1*VEL_1+VEL_2*VEL_2) %>%
  filter(vel > 1e7) %>%
  mutate(disp = floor((abs(DISPL_0)+abs(DISPL_1)+abs(DISPL_2)))) %>%
  select(vel, disp) %>%
  group_by(disp) %>% 
  summarize(avel = mean(vel)) %>%
  collect() %>%
  ggplot(aes(disp, avel)) + geom_bar(stat="identity")
```

A summary of what I did:

- load t
- add a column that is velocity squared
- filter where velocity squared is > 1e7
- add a column that discretizes displacement into integers
- select only the two new columns
- group by displacement
- get the average velocity per displacement group
- collect the data
- create a ggplot
- map to bars
- (display data)

Let's go on and show one more feature of R and its graphics
ecosystem, Shiny. The following code is taken from the Shiny
example web page, but applied to our can data. Again, Shiny
is similar to Bokeh. Plotly and Tableau are commercial 
solutions similar to these to deploy web visualizations 
(though plotly itself is open source technology).


```{r shiny-example}
require(shiny)

s <- t %>% filter(Points_2 > -6) %>% filter(Points_2 < -4) %>% collect()

ui <- fluidPage(
  # Some custom CSS for a smaller font for preformatted text
  tags$head(
    tags$style(HTML("
      pre, table.table {
        font-size: smaller;
      }
    "))
  ),

  fluidRow(
    column(width = 4, wellPanel(
      radioButtons("plot_type", "Plot type",
        c("base", "ggplot2")
      )
    )),
    column(width = 4,
      # In a plotOutput, passing values for click, dblclick, hover, or brush
      # will enable those interactions.
      plotOutput("plot1", height = 350,
        # Equivalent to: click = clickOpts(id = "plot_click")
        click = "plot_click",
        dblclick = dblclickOpts(
          id = "plot_dblclick"
        ),
        hover = hoverOpts(
          id = "plot_hover"
        ),
        brush = brushOpts(
          id = "plot_brush"
        )
      )
    )
  ),
  fluidRow(
    column(width = 3,
      verbatimTextOutput("click_info")
    ),
    column(width = 3,
      verbatimTextOutput("dblclick_info")
    ),
    column(width = 3,
      verbatimTextOutput("hover_info")
    ),
    column(width = 3,
      verbatimTextOutput("brush_info")
    )
  )
)


server <- function(input, output) {
  output$plot1 <- renderPlot({
    if (input$plot_type == "base") {
      plot(s$Points_0, s$Points_2)
    } else if (input$plot_type == "ggplot2") {
      ggplot(s, aes(Points_0, Points_2)) + geom_point()
    }
  })

  output$click_info <- renderPrint({
    cat("input$plot_click:\n")
    str(input$plot_click)
  })
  output$hover_info <- renderPrint({
    cat("input$plot_hover:\n")
    str(input$plot_hover)
  })
  output$dblclick_info <- renderPrint({
    cat("input$plot_dblclick:\n")
    str(input$plot_dblclick)
  })
  output$brush_info <- renderPrint({
    cat("input$plot_brush:\n")
    str(input$plot_brush)
  })

}


shinyApp(ui, server)
```

You probably won't see anything unless you are running the
interactive version of this document.

As you can see, we are able to get the same sort of
interactive graphics that is available through Bokeh in Python,
and D3 and the like in Javascript all in one ecosystem.

If I provided more controls, I could change it such that a
user within the application could change the filtering on
the data set, which is done at the SQL/file layer, while
the additional graphics transformations are done while computing.

Let's head back to the Jupyter notebook to finish up the slides.
